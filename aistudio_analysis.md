1. Critical Errors & Build Configuration
CMake Dependency Mismatch (GMP vs. Boost): The CMakeLists.txt file explicitly searches for and links the GMP library (libgmp, libgmpxx). However, include/rational_linalg/rational.hpp has commented out the GMP implementation and actively uses boost::multiprecision::cpp_rational. This creates a useless dependency on GMP for the build system while the code actually runs on a generic C++ backend. This will cause linker bloat or build failures on systems that have Boost but lack GMP.
Hardcoded Build Type: The CMake configuration forcibly sets CMAKE_BUILD_TYPE to "Release" and overrides user flags. This prevents developers from building in "Debug" or "RelWithDebInfo" modes, making debugging segmentation faults or logic errors extremely difficult.
Exception Type Mismatch Risk: In src/fracessa.cpp, the code catches std::system_error to detect overflows from small_rational (which uses boost::safe_numerics). While Boost.SafeNumerics does throw exceptions derived from std::system_error by default, this is an implementation detail. If the underlying policy of the safe integer type changes (e.g., to trap_exception), the fallback to arbitrary precision will fail, causing the program to crash instead of upgrading precision.
2. Logic & Algorithmic Issues
Exception-Based Control Flow: The mechanism for switching from small_rational to arbitrary_precision relies on "Look Before You Leap" (LBYL) failure. It attempts a calculation, catches an overflow exception, and then restarts with higher precision. Exceptions are computationally expensive in C++. Since the matrix structure implies whether numbers will grow large (e.g., via determinant calculations), a heuristic check or a direct use of arbitrary precision for difficult matrices would be cleaner and safer.
Bareiss Algorithm on Rationals: The code uses the Bareiss fraction-free algorithm for rational types. Bareiss is optimized for Integers to avoid floating-point division. When applied to rational classes that automatically reduce fractions (GCD calculation on every operation), Bareiss is often slower than standard Gaussian elimination because it allows intermediate numerators/denominators to grow much larger than necessary before the final division step, while incurring the cost of GCD reductions at every step anyway.
Potential Infinite Loop in Constraint Check: In checkstab.cpp (and findeq.cpp), the logic relies on comparison tolerances for double types, but exact comparisons for rational types. If a solution lies exactly on the boundary due to precision loss in the double check, it might pass the double check but fail the rational reconstruction or vice versa, potentially leading to inconsistent states if the code logic toggles between the two representations.
Bitset Rotation Logic: In bitset64.hpp, the rot_one_right function manually implements rotation for a subset of bits (nbits). If nbits is 0 or 1, the behavior of shifts nbits - 1 might be undefined or logically incorrect depending on the input, though dimension_ usually protects this.
3. Performance Optimizations
Single-Threaded Execution: The most significant bottleneck is that fracessa.cpp iterates through supports sequentially. The search for ESS candidates is an "embarrassingly parallel" problem. Using OpenMP or std::async to process support sizes or individual supports in parallel would yield a linear speedup proportional to CPU cores.
Matrix Value Semantics: The custom Matrix class implements arithmetic operators (+, -, *) by returning new Matrix objects by value. This triggers memory allocations for every intermediate operation (e.g., A = B + C - D creates multiple temporary matrices). Implementing move semantics is present, but arithmetic operators should ideally use expression templates (like Eigen) or at least op= implementations to minimize allocation overhead.
Safe Integer Overhead: The small_rational type uses safe<int64_t>. While safe, this adds branch checks to every single arithmetic operation (add, sub, mul). Given that the code wraps the high-level logic in a try-catch block anyway, using raw int64_t with compiler built-in overflow checks (e.g., __builtin_mul_overflow) only at critical points (like determinant calculation) would be significantly faster.
Redundant GMP Linking: As noted in errors, linking libgmp but using cpp_int is a performance miss. boost::multiprecision::gmp_rational is generally faster than cpp_rational because it uses the highly optimized assembly of the GMP library. If GMP is linked anyway, the code should actually use the GMP backend for rational to gain performance.
4. Code Quality & Maintainability
Dead Code Inclusion: The repository contains a src/zzz_old and include/zzz_old directory containing deprecated Eigen extensions and older logic. These are included in the compilation list in fracessa.cpp (via #include of headers) or just sitting in the source tree, confusing the build process and static analysis.
Fragile String Parsing: The manual string parsing in main.cpp (finding #, then splitting by ,) is brittle. It assumes specific formatting and offers poor error reporting for malformed inputs (e.g., trailing commas, extra spaces).
Header Hygiene: fracessa.hpp includes spdlog and Boost headers. Any file including fracessa.hpp pulls in these heavy dependencies, increasing compilation time. Forward declarations or the Pimpl idiom should be used to isolate implementation details like logging.
Namespace Pollution: The rational_linalg namespace is good, but there are instances of using namespace logic or type aliases defined in the global scope (e.g., using RationalMatrix = ...) inside headers, which pollutes the global namespace for any user of the library.
Lack of Unit Tests: There is a tests folder mentioned in CMake but commented out. A math-heavy library relying on custom linear algebra implementations requires rigorous unit testing for edge cases (singular matrices, zero matrices, varying sparsity).
5. Specific File Issues
findeq.cpp: The check_constraints function recalculates row sums from scratch. This is an 
 operation. If this is called frequently within a loop, it should be optimized via matrix-vector multiplication if possible.
supports.hpp: The remove_supersets function uses std::remove_if on a vector. For vectors, this is 
 where 
 is the number of remaining supports. Since this is called frequently, using a more appropriate data structure (like a prefix tree/Trie or a set of bitsets) could optimize the pruning process.